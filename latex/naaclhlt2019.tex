%
% File naacl2019.tex
%
%% Based on the style files for ACL 2018 and NAACL 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper, french]{article}
\usepackage[hyperref]{naaclhlt2019}
\usepackage{times}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{latexsym}
\usepackage{tikz}
\usetikzlibrary{arrows}

\makeatletter
\pgfdeclareshape{datastore}{
  \inheritsavedanchors[from=rectangle]
  \inheritanchorborder[from=rectangle]
  \inheritanchor[from=rectangle]{center}
  \inheritanchor[from=rectangle]{base}
  \inheritanchor[from=rectangle]{north}
  \inheritanchor[from=rectangle]{north east}
  \inheritanchor[from=rectangle]{east}
  \inheritanchor[from=rectangle]{south east}
  \inheritanchor[from=rectangle]{south}
  \inheritanchor[from=rectangle]{south west}
  \inheritanchor[from=rectangle]{west}
  \inheritanchor[from=rectangle]{north west}
  \backgroundpath{
    %  store lower right in xa/ya and upper right in xb/yb
    \southwest \pgf@xa=\pgf@x \pgf@ya=\pgf@y
    \northeast \pgf@xb=\pgf@x \pgf@yb=\pgf@y
    \pgfpathmoveto{\pgfpoint{\pgf@xa}{\pgf@ya}}
    \pgfpathlineto{\pgfpoint{\pgf@xb}{\pgf@ya}}
    \pgfpathmoveto{\pgfpoint{\pgf@xa}{\pgf@yb}}
    \pgfpathlineto{\pgfpoint{\pgf@xb}{\pgf@yb}}
 }
}
\makeatother

\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission
\def\aclpaperid{***} %  Enter the acl Paper ID here

\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Projet INF8460 Automne 2019 }

\author{Samuel Ferron \\
  1843659 \\
  {\tt } \\\And
  Jean-Frédéric Fontaine \\
  1856632 \\
  {\tt} \\\And
  Mathieu B\'eligon \\
  2032839\\
  {\tt } \\}


\date{}

\begin{document}
\maketitle
%\begin{Intro}
 %This document contains the instructions for preparing a camera-ready
 % manuscript for the proceedings of NAACL-HLT 2019. The document itself
 % conforms to its own specifications, and is therefore an example of
 % what your manuscript should look like. These instructions should be
 % used for both papers submitted for review and for final versions of
 %accepted papers.  Authors are asked to conform to all the directions
 % reported in this document.
%\end{Intro}

\section{Introduction}

Les dernières années nous ont permis de témoigné d’avancées fulgurantes dans le domaine du traitement automatique de la langue naturel. Plus particulièrement, les machines sont désormais capables de performer aussi bien sinon mieux que les humains dans plusieurs tâches comme la traduction, la classification et la génération de texte. Ces avancées sont en grande partie expliquées par l’évolution très rapide des techniques utilisées pour réaliser de telles tâches. En effet, depuis l’élaboration des Bag-Of-Words, plusieurs techniques de plus en plus raffinées comme les RNN et les LSTM ont vue le jour \cite{lstm}. Cependant, ces techniques récursives se sont retrouver dans l’ombre des nouveaux modèles faisant appel aux Transformers qui contiennent maintenant des modules d’attention. Ce concept d’attention peux maintenant être intégré tant dans des encodeurs que des décodeurs afin d’augmenter considérablement les performances de modèles de génération de textes. Par contre, la véritable révolution provient d’une équipe de google qui ont utilisé le modèle d’encodeur du Tranformer afin de créer une architecture composée de mécanismes d’attention bidirectionnels multicouche : BERT. Dans ce papier, nous décrirons en détails comment nous avons utilisé ce modèle ainsi que certaines de ses variantes afin de résoudre le problème Kaggle dans le cadre du cours INF8460 : traitement automatique de langue naturelle. Brièvement, il nous a été demandé de créer un modèle de régression capable d’évaluer la similarité entre deux phrases. Pour ce faire, nous avons créer un ensemble composé d’un BERT un ROBERTA et un XLNET. Nous avons ensuite inséré les résultats générés par ces modèles dans un feedfoward network afin de produire nos prédiction finales. À titre comparatif, nous avons aussi simplement moyenné ces résultats afin de générer nos prédictions. Nous avons réussi à atteindre un taux de classification de 90.0641, ce qui au moment de la rédaction de ce papier nous garantissait une première place. Ce résultat correspond aussi à l'état de l'art pour cette tâche. 

%\section{Introduction}
%
%The following instructions are directed to authors of papers submitted
%to NAACL-HLT 2019 or accepted for publication in its proceedings. All
%authors are required to adhere to these specifications. Authors are
%required to provide a Portable Document Format (PDF) version of their
%papers. \textbf{The proceedings are designed for printing on A4
%paper.}

\section{Méthodologie}


\subsection{Définition de la tâche/objectif}
Le but de l’exercice est de définir un modèle afin de prédire la similarité sémantique entre deux phrases, comme décrit la tâche Semantic Textual Similarity (STS). La motivation de STS est d’être en moyen de construire une représentation du sens d’une phrase. L’hypothèse est que cette représentation peut être utilisé pour diverses applications comme la traduction machine (MT), la question-réponse (QA), la recherche sémantique et bien d’autres. En effet, la comparaison sémantique entre deux phrases en elle même peut sembler une tâche futile, mais la méthodologie que nous avons implémentée pourra être utiliser dans diverses applications. L’ensemble de données avec lequel nous avons travaillés est STSbenchmark, l’ensemble de référence pour STS. L’ensemble contient 5749 d’enregistrements d'entraînement et 1379 de test. Chaque données contient deux phrases, un identifiant et un score de similarité situé entre 0 et 5 seulement pour les données d'entraînement. Un score de 5 indique que les deux phrases sont sémantiquement identique, 0 indique qu’il n’ watatatow


\subsection{Définition de l'algorithme/méthode/technique}
À faire.

\subsubsection{Stacking des models}

A ce stade, nous avons donc 3 modèles, issus du fine-tuning de models state-of-the-art. Nous avons alors tente une methode de stacking, pour combiner les resultats des trois modeles. Notre intuition était que les modeles devaient apprendre differement, et qu'en combinant leurs predictions nous ameliorerions leurs resultats.

Nous avons tenté 2 approches différentes pour reunir les models:

\begin{itemize}
  \item Faire une moyenne des predictions des modeles;
  \item Entrainer un réseau de neurones fast-forward, avec comme entrée les prédictions des autres modèles.
\end{itemize}

La première methode envisagé ne nécessitant pas plus d'explications, nous allons nous attarder sur la seconde.

\paragraph{Méthodologie} Nous avons divisé nos données en deux ensembles: un premier, plus volumineux, sur lequel nous avons entrainé les 3 models SOTA, et un second, plus petit, sur lequel nous avons prédits les resultats des 3 sous-models, avant d'utiliser ces prédictions pour entrainer le modèle de stacking.

\paragraph{Architecture} Nous avons utilisé un modèle assez simple, avec une couche cachée (figure \ref{fig:stacking:archi}). Notre intention étant d'avoir un modèle non-linéaire (sinon cela revenait à faire une moyenne pondérée), mais pas trop complexe, afin de pouvoir entrainer le modèle avec peu de données sans risque d'overfitting.

\begin{figure}
\begin{center}
\begin{tikzpicture}[
  font=\sffamily,
  every matrix/.style={ampersand replacement=\&,column sep=.2cm,row sep=.3cm},
  layer/.style={draw,thick,rounded corners,fill=gray!3},
  data/.style={draw,shape=datastore,inner sep=.3cm},
  output/.style={draw,thick,circle,fill=green!20},
  model/.style={draw,very thick,fill=gray!20},
  dots/.style={gray,scale=2},
  to/.style={->,>=stealth',shorten >=1pt,semithick,font=\sffamily\footnotesize},
  every node/.style={align=center,inner sep=.2cm}]

  % Position the nodes using a matrix layout
  \matrix{
    \& \node[output] (score) {s}; \& \\

    \& \& \& \\

    \& \node[layer] (output_layer) {Output Layer}; \& \\
    \& \node[layer] (hidden_layer) {Hidden Layer}; \& \\

    \& \& \& \\

    \& \node[data] (sub_scores) {s1 | s2 | s3}; \& \\

    \& \& \& \\

    \node[model] (bert) {BERT};  \& \node[model] (roberta) {RoBERTa}; \& \node[model] (xlnet) {XLNet}; \\

    \& \& \& \\

    \& \node[data] (sentences) {input sentences}; \& \\
  };

  % Draw the arrows between the nodes and label them.
  \draw[to] (output_layer) -- (score);
  \draw[to] (hidden_layer) -- (output_layer);

  \draw[to] (sub_scores) -- (hidden_layer);

  \draw[to] (bert) -- node[midway,left] {s1} (sub_scores);
  \draw[to] (roberta) -- node[midway,left] {s2} (sub_scores);
  \draw[to] (xlnet) -- node[midway,right] {s3} (sub_scores);

  \draw[to] (sentences) -- (bert);
  \draw[to] (sentences) -- (roberta);
  \draw[to] (sentences) -- (xlnet);
\end{tikzpicture}
\caption{Architecture du modèle de stacking} \label{fig:stacking:archi}
\end{center}
\end{figure}

\subsection{Évaluation expérimentale }

À faire.

\subsection{Ensemble de données, métriques et baselines }

À faire. 


%\begin{table}[t!]
%\begin{center}
%\begin{tabular}{|l|rl|}
%\hline \bf Type of Text & \bf Font Size & \bf Style \\ \hline
%paper title & 15 pt & bold \\
%author names & 12 pt & bold \\
%author affiliation & 12 pt & \\
%the word ``Abstract'' & 12 pt & bold \\
%section titles & 12 pt & bold \\
%document text & 11 pt  &\\
%captions & 10 pt & \\
%abstract text & 10 pt & \\
%bibliography & 10 pt & \\
%footnotes & 9 pt & \\
%\hline
%\end{tabular}
%\end{center}
%\caption{\label{font-table} Font guide. }
%\end{table}

\subsection{Résultats}


\subsection{Discussion}


\section{Travaux connexes}



\section{Travaux futures et conclusion }

\section*{Acknowledgments}
 

%where \verb|naaclhlt2019| corresponds to a naaclhlt2019.bib file.
%\bibliography{naaclhlt2019}
%\bibliographystyle{acl_natbib}

\appendix





\end{document}
